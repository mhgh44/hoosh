
# خلاصه جامع – حافظه‌های تداعی و شبکه‌های هاپفیلد  
*(Associative Memories & Hopfield Networks – condensed study guide)*  

---

## 1. حافظهٔ تداعی (Associative Memory)  
**ایدهٔ اصلی:** شبکه‌ای از نورون‌های ساده بتواند با دیدن نسخهٔ ناقص یا نویزیِ یک الگو، نسخهٔ کامل را «تداعی» و بازسازی کند.  
- **خروجی = یادآوری (Recall)**؛ ورودی معمولاً الگوی خراب‌شده است.  
- دو نوع مهم:  
  1. **Hetero‑Associative** (خروجی با ابعاد یا معنای متفاوت از ورودی)  
  2. **Auto‑Associative** (ورودی و خروجی یکسان؛ برای **Pattern Completion**)  

### قانون هب (Hebbian) برای ذخیره‌سازی  
برای هر جفت ورودی–خروجی \((\mathbf{x},\mathbf{y})\):  
\[
W \;{+}{=}\; \mathbf{y}\,\mathbf{x}^T
\]  
- اگر از بردارهای **بای‌پولار** (+1/‑1) استفاده شود و قطر \(W\) صفر گردد ➜ توانِ اصلاح خطا بیشتر می‌شود.  
- جمع چند outer‑product باعث «تداخل» (cross‑talk) می‌شود؛ ظرفیت شبکه محدود است.

---

## 2. شبکهٔ هاپفیلد گسسته (Discrete Hopfield Model, DHM)  
| ویژگی | توضیح کوتاه |
|-------|-------------|
| معماری | یک لایه با **اتصالات کامل، متقارن، بدون وزنِ خود** \(w_{ii}=0\) |
| حالت نورون | دودویی یا بای‌پولار \(\{+1,-1\}\)؛ تابع آستانه (hard‑limit) |
| یادگیری | *معمولاً* قوانین هب یا مشتق آن برای ساخت \(W\) |
| فراخوانی | به‑روزرسانی **نامتقارن (asynchronous)** بهتر تضمین همگرایی است؛ مدل همزمان سریع‌تر ولی ممکن است نوسان کند |
| همگرایی | از طریق تابع **انرژی**:  \[E = -	frac12 \sum_{i
eq j} w_{ij}x_i x_j + \sum_i 	heta_i x_i\]  هر تغییرِ مجاز انرژی را کم می‌کند ➜ شبکه به نزدیک‌ترین مینیمم محلی می‌رسد.|

### حافظه‌های «واقعی» و «کاذب»  
- **Genuine**: دقیقاً همان الگوهای ذخیره‌شده؛  
- **Spurious**: مینیمم‌هایی که حاصل تداخل الگوها هستند؛ ممکن است الگوی اشتباه بازیابی شود.

---

## 3. ظرفیت ذخیره‌سازی  
- برای الگوهای **تصادفی**: تقریباً \(0.15\,n\) الگو در شبکه‌ای با \(n\) نورون قابل بازیابی پایدار است.  
- برای الگوهای **متحامد (Orthogonal)**: تا \(n-1\) الگو بدون خطا ذخیره می‌شوند (ثابتِ تئوری).  

---

## 4. شبکهٔ هاپفیلد پیوسته (Continuous Hopfield / Hopfield–Tank)  
معادلهٔ دینامیک هر نورون:  
\[
	au rac{du_i}{dt} = -u_i + \sum_j w_{ij}v_j + I_i
\]  
\[
v_i = f(u_i)\quad (f: 	ext{sigmoid یا tanh})
\]  
- **همهٔ نورون‌ها همزمان** مقدارشان را به‌روز می‌کنند.  
- تابع انرژی پیوسته‌ای وجود دارد که **گرادیان منفی** آن مسیر تغییر شبکه است؛ بنابراین سیستم در داخل هایپرباکس حرکت کرده و به مینیمم محلی می‌رسد.  

---

## 5. کاربرد در بهینه‌سازی ترکیبیاتی (مثال: مسئلهٔ فروشندهٔ دوره‌گرد – TSP)  
1. **کدگذاری**: ماتریس \(N{	imes}N\)؛ خانهٔ \(v_{i,t}=1\) یعنی «شهر \(i\) در موقعیت \(t\) از تور است».  
2. **قیدها** به‌عنوان جریمه در انرژی اضافه می‌شود (هر شهر دقیقاً یک‌بار در هر ستون/سطر).  
3. **انرژی کل** = طول مسیر + ضرایب جریمه.  
4. سیر دینامیک هاپفیلد (یا مدل Hopfield–Tank) مینیمم انرژی ≈ مسیر کوتاه.  

**مزایا:** محاسبات کاملاً موازی، اجرا روی سخت‌افزار آنالوگ سریع.  
**معایب:** گیرکردن در مینیمم‌های محلی، تنظیم ضرایب جریمه حساس، مقیاس‌پذیری محدود.  

---

## 6. نکات کلیدی امتحان  
1. تعریف حافظهٔ تداعی و تفاوت **auto** vs **hetero**.  
2. فرمول وزن هب با بای‌پولار و چرا قطر باید صفر شود.  
3. فرآیند به‌روزرسانی در DHM و دلیل نیاز به تقارن وزن‌ها.  
4. تابع انرژی DHM و اثبات کاهش یکنوا.  
5. تفاوت DHM (گسسته) و CHM (پیوسته) – معادلات، همگرایی.  
6. مفهوم **basin of attraction**، «حافظهٔ کاذب»، و اثر ترتیب به‌روزرسانی.  
7. محدودهٔ ظرفیت ذخیره و نقش الگوهای متعامد.  
8. نحوهٔ نگاشت مسائل بهینه‌سازی (مانند TSP) به شبکهٔ هاپفیلد و چالش‌های آن.  

---

### جمع‌بندی سریع  
- **یادگیری = ساخت وزن‌ها** (معمولاً یک‌مرحله‌ای)؛ **یادآوری = دینامیک شبکه**.  
- حضور تابع انرژی تضمین می‌کند **همگرایی** ولی نه لزوماً بهترین پاسخ.  
- شبکهٔ هاپفیلد، فراتر از حافظه، **چارچوبی فیزیکی** برای حل تقریبـی مسائل NP‑سخت فراهم می‌کند.  

**موفق باشید؛ امتحان آسان!**
